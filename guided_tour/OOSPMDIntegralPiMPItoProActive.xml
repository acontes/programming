<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/css" href="../viewDocbook.css"?>
<chapter id="OOSPMDJacobiexample">
  <title>SPMD PROGRAMMING</title>

  <sect1>
    <title>OO SPMD on a Integral Pi example MPI to ProActive
    adaptation</title>

    <sect2>
      <title>Introduction</title>

      <sect3>
        <title>Description of the IntegralPi example</title>

        <para>In this chapter we are going to see a simple example of an MPI
        written program ported to ProActive.</para>

        <para>First let's introduce what we are going to compute.</para>

        <para>This simple program approximates <ulink
        url="http://en.wikipedia.org/wiki/Pi">pi</ulink> by computing :</para>

        <para>pi = <emphasis role="bold">integral</emphasis> from <emphasis
        role="bold">0</emphasis> to <emphasis role="bold">1</emphasis> of
        <emphasis role="bold"><emphasis role="bold">4</emphasis>/( 1+x*x )
        dx</emphasis></para>

        <para>Which is approximated by :</para>

        <para><emphasis role="bold">sum</emphasis> from <emphasis
        role="bold">k=1</emphasis> to <emphasis role="bold">N</emphasis> of
        <emphasis role="bold">4 / ( ( 1 +( k-1/2 ) **2 )</emphasis></para>

        <para>The only input data required is N, the number of
        iterations.</para>

        <para>Involved files :</para>

        <itemizedlist>
          <listitem>
            <para>int_pi2.c : the original MPI implementation </para>
          </listitem>

          <listitem>
            <para>Launcher.java : the main class</para>
          </listitem>

          <listitem>
            <para>Worker.java : the class implementing the SPMD code</para>
          </listitem>
        </itemizedlist>
      </sect3>
    </sect2>

    <sect2>
      <title>Initialization</title>

      <sect3>
        <title>MPI Initalization primitives</title>

        <para>Some basic primitives are used, notice that MPI provides a rank
        to each process and the group size ( the number of involved processes
        ).</para>

        <programlisting>  // All instances call startup routine to get their instance number (mynum) 
  MPI_Init(&amp;argc, &amp;argv);
  MPI_Comm_rank(MPI_COMM_WORLD, &amp;mynum);
  MPI_Comm_size(MPI_COMM_WORLD, &amp;nprocs);

  // Get a value for N
  solicit (&amp;N, &amp;nprocs, mynum);</programlisting>
      </sect3>

      <sect3>
        <title>ProActive Initialization primitives</title>

        <para>First we need to create the group of workers (MPI processes
        represented by active objects). Notice that the creation of active
        objects is done in Launcher.java.</para>

        <para>The group of active objects is created using specified
        parameters and the nodes specified in the deployement
        descriptor.</para>

        <programlisting>  // Group creation           
  Worker workers = (Worker) ProSPMD.newSPMDGroup(
         Worker.class.getName(), params, provideNodes(args[0]));

  // Once the group is created and the value for N is entered we can start the workers job
  // Workers starts their job and return a group of Futures
  DoubleWrapper results = workers.start( numOfIterations );</programlisting>

        <para>The ProSPMD layer provides similar to MPI initialization
        primitives. In Worker.java you can identify this initialisation. Note
        that one-to-one communications will be done thanks to an array view on
        the created group.</para>

        <programlisting>  // Worker initialization
  rank = ProSPMD.getMyRank();
  groupSize = ProSPMD.getMySPMDGroupSize();

  // Get all workers references
  workersArray = (Worker[]) ProActiveGroup.getGroup(ProSPMD.getSPMDGroup()).toArray(new Worker[0]);</programlisting>
      </sect3>
    </sect2>

    <sect2>
      <title>Communication primitives</title>

      <sect3>
        <title>Communication pattern</title>

        <para>The communication pattern is very simple, it's done in 2 steps.
        First the process 0 Broadcasts N then waits for the result from each
        other process and sums the received values.</para>

        <figure>
          <title>Communication pattern - step 1</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="guided_tour/pics/nbody/MPI_step1.png"
                         format="PNG" />
            </imageobject>
          </mediaobject>
        </figure>

        <figure>
          <title>Communication pattern -step 2</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="guided_tour/pics/nbody/MPI_step1.png"
                         format="PNG" />
            </imageobject>
          </mediaobject>
        </figure>
      </sect3>

      <sect3>
        <title>MPI Approach</title>

        <para>The MPI implementation involves 3 communication primitives
        :</para>

        <itemizedlist>
          <listitem>
            <para><emphasis role="bold">MPI_Send</emphasis> ( Sends data to
            one process )</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">MPI_Recv</emphasis> ( Receives data
            from a sending process )</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">MPI_Bcast</emphasis> ( Broadcast a
            data to all processes )</para>
          </listitem>
        </itemizedlist>

        <para>Please note that MPI_Bcast, MPI_Send and MPI_Recv primitives are
        blocking.</para>

        <programlisting>  // Get a value of N from stdin for the next run and Broadcast it
  <emphasis role="bold">MPI_Bcast</emphasis>(pN, 1, MPI_INT, source, MPI_COMM_WORLD);

  // LOCAL COMPUTATION LOOP
  // ... 

  if ( mynum == 0 ) { // Check if i'm the leader process
     for (i=1; i&lt;nprocs; i++) {
        source = i;
        info = <emphasis role="bold">MPI_Recv</emphasis>(&amp;x, 1, MPI_FLOAT, source, type, MPI_COMM_WORLD, &amp;status); // waits the value from source process
        sum=sum+x; // sum up the receive value
     }
  } else {
     info = <emphasis role="bold">MPI_Send</emphasis>(&amp;sum, 1, MPI_FLOAT, dest, type, MPI_COMM_WORLD); // if i'm not the process 0 i send my sum      
  }  </programlisting>
      </sect3>

      <sect3>
        <title>ProActive Approach</title>

        <para>The ProActive implementation is quite similar to MPI one. The
        fact is that all communications in ProActive are asynchronous (
        non-blocking ) by default, therefore we need to specify explicitely to
        block until a specific request. </para>

        <programlisting>  // The leader collects partial results.
  // Others just send their computed data to the rank 0.
  
  if ( rank==0 ) { // Check if i'm the leader worker 
     for ( i=1; i&lt;groupSize; i++ ) {
         <emphasis role="bold">body.serve(body.getRequestQueue().blockingRemoveOldest("updateX"))</emphasis>; // block until an updateX call
         sum += x;
     }            
  } else {
     <emphasis role="bold">workersArray[0].updateX(sum)</emphasis>;
  }</programlisting>

        <para>The leader blocks his request queue until another worker will do
        a distant call on the leader's <emphasis
        role="bold">updateX</emphasis> method which is : </para>

        <programlisting>  public void updateX(double value){
     this.x = value;
  } </programlisting>
      </sect3>
    </sect2>

    <sect2>
      <title>Running ProActive example</title>

      <sect3>
        <title>Compilation</title>

        <para>ProActive distribution comes with scripts to easily recompile
        the provided examples:</para>

        <screen>linux&gt;ProActive/compile/build</screen>

        <para>or</para>

        <screen>windows&gt;ProActive/compile/build.bat</screen>

        <para>Use the build script to recompile the example</para>

        <screen>build examples</screen>

        <para>2 source files must appear as being recompiled.</para>

        <para>Following the recompilation, rerun the examples as explained in
        section 1.2 above, and observe the differences.</para>
      </sect3>

      <sect3>
        <title>Running ProActive example</title>

        <para>In ProActive/scripts/unix or windows run integralpi.sh, you can
        specify the number of workers from the command line.</para>

        <screen>bash-3.00$ ./integralpi.sh

--- IntegralPi --------------------------------------------------
The number of workers is 4
 --&gt; This ClassFileServer is reading resources from classpath 2011
Created a new registry on port 1099
ProActive Security Policy (proactive.runtime.security) not set. Runtime Security disabled
************* Reading deployment descriptor: file:./../../descriptors/Matrix.xml ********************
created VirtualNode name=matrixNode
**** Starting jvm on amda.inria.fr
**** Starting jvm on amda.inria.fr
**** Starting jvm on amda.inria.fr
ProActive Security Policy (proactive.runtime.security) not set. Runtime Security disabled
 --&gt; This ClassFileServer is reading resources from classpath 2012
ProActive Security Policy (proactive.runtime.security) not set. Runtime Security disabled
ProActive Security Policy (proactive.runtime.security) not set. Runtime Security disabled
 --&gt; This ClassFileServer is reading resources from classpath 2013
 --&gt; This ClassFileServer is reading resources from classpath 2014
**** Starting jvm on amda.inria.fr
Detected an existing RMI Registry on port 1099
Detected an existing RMI Registry on port 1099
Detected an existing RMI Registry on port 1099
ProActive Security Policy (proactive.runtime.security) not set. Runtime Security disabled
 --&gt; This ClassFileServer is reading resources from classpath 2015
//amda.inria.fr/matrixNode2048238867 successfully bound in registry at //amda.inria.fr/matrixNode2048238867
**** Mapping VirtualNode matrixNode with Node: //amda.inria.fr/matrixNode2048238867 done
//amda.inria.fr/matrixNode690267632 successfully bound in registry at //amda.inria.fr/matrixNode690267632
**** Mapping VirtualNode matrixNode with Node: //amda.inria.fr/matrixNode690267632 done
//amda.inria.fr/matrixNode1157915128 successfully bound in registry at //amda.inria.fr/matrixNode1157915128
**** Mapping VirtualNode matrixNode with Node: //amda.inria.fr/matrixNode1157915128 done
Detected an existing RMI Registry on port 1099
//amda.inria.fr/matrixNode-814241328 successfully bound in registry at //amda.inria.fr/matrixNode-814241328
**** Mapping VirtualNode matrixNode with Node: //amda.inria.fr/matrixNode-814241328 done
4 nodes found
Generating class : pa.stub.org.objectweb.proactive.examples.integralpi.Stub_Worker

Enter the number of iterations (0 to exit) : 100000
Generating class : pa.stub.org.objectweb.proactive.examples.integralpi.Stub_Worker
Generating class : pa.stub.org.objectweb.proactive.examples.integralpi.Stub_Worker
Generating class : pa.stub.org.objectweb.proactive.examples.integralpi.Stub_Worker
Generating class : pa.stub.org.objectweb.proactive.examples.integralpi.Stub_Worker

         Worker 2 Calculated x = 0.7853956634245252 in 43 ms


         Worker 3 Calculated x = 0.7853906633745299 in 30 ms


         Worker 1 Calculated x = 0.7854006634245316 in 99 ms


         Worker 0 Calculated x = 3.141592653598117 in 12 ms


Calculated PI is 3.141592653598117 error is 8.324008149429574E-12

Enter the number of iterations (0 to exit) :  </screen>
      </sect3>
    </sect2>
  </sect1>
</chapter>